{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6150906b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T08:41:38.868250Z",
     "start_time": "2025-01-28T08:41:37.447610Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim, autograd\n",
    "from torch.nn import functional as F\n",
    "from pyDOE import lhs\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from utils_training import *\n",
    "\n",
    "#Paper reproduction\n",
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597616bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T08:41:42.519715Z",
     "start_time": "2025-01-28T08:41:38.870857Z"
    }
   },
   "outputs": [],
   "source": [
    "N_train = 1000\n",
    "N_bound = 400\n",
    "N_init = 200\n",
    "\n",
    "# Input  [x,t]\n",
    "# Output [theta, w]\n",
    "la = np.array([np.pi,1])\n",
    "lb = np.array([0,0])\n",
    "\n",
    "traindata = lb+(la-lb)*lhs(2,N_train)\n",
    "x_inside = traindata[:,0:1]\n",
    "t_inside = traindata[:,1:2]\n",
    "\n",
    "x_inside = numpy_to_tensor(x_inside, var_name=\"x_inside\", value_range_dim = True, to_torch = True, to_cuda = True, requires_grad = True)\n",
    "t_inside = numpy_to_tensor(t_inside, var_name=\"t_inside\", value_range_dim = True, to_torch = True, to_cuda = True, requires_grad = True)\n",
    "\n",
    "x_bound = lb[0]+(la[0]-lb[0])*lhs(1,N_bound)\n",
    "x_bound_t_zero = np.zeros_like(x_bound)\n",
    "t_bound = lb[1]+(la[1]-lb[1])*lhs(1,N_init)\n",
    "\n",
    "\n",
    "x_bound = numpy_to_tensor(x_bound, var_name=\"x_bound\", value_range_dim = True, to_torch = True, to_cuda = True, requires_grad = True)\n",
    "x_bound_t_zero = numpy_to_tensor(x_bound_t_zero, var_name=\"x_bound_t_zero\", value_range_dim = True, to_torch = True, to_cuda = True, requires_grad = True)\n",
    "t_bound = numpy_to_tensor(t_bound, var_name=\"t_bound\", value_range_dim = True, to_torch = True, to_cuda = True, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb70a173",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T08:41:42.528912Z",
     "start_time": "2025-01-28T08:41:42.523306Z"
    }
   },
   "outputs": [],
   "source": [
    "# External force\n",
    "def g(data):\n",
    "    x = data[:,0:1]\n",
    "    t = data[:,1:2]\n",
    "    return np.cos(t) - (np.pi / 2) * np.sin(x) * np.cos(t)\n",
    "\n",
    "# Exact solution\n",
    "def exact_theta(data):\n",
    "    x = data[:,0:1]\n",
    "    t = data[:,1:2]\n",
    "    return (np.pi / 2 * np.cos(x) + (x - np.pi / 2)) * np.cos(t)\n",
    "\n",
    "def exact_w(data):\n",
    "    x = data[:,0:1]\n",
    "    t = data[:,1:2]\n",
    "    return (np.pi / 2) * np.sin(x) * np.cos(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437840cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T08:41:42.543438Z",
     "start_time": "2025-01-28T08:41:42.531435Z"
    }
   },
   "outputs": [],
   "source": [
    "random_seed = 1234\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "x_values = [0.2, 0.8, 1.8, 2.6, 3.0]\n",
    "observe_number_per_x = 10\n",
    "\n",
    "observe_data = []\n",
    "for x in x_values:\n",
    "    t_for_x = lhs(1, observe_number_per_x)\n",
    "    x_column = np.full((observe_number_per_x, 1), x)\n",
    "    observe_data_x_t = np.hstack((x_column, t_for_x))\n",
    "    observe_data.append(observe_data_x_t)\n",
    "\n",
    "observe_data = np.vstack(observe_data)\n",
    "\n",
    "observe_clear_theta = exact_theta(observe_data)\n",
    "observe_clear_w = exact_w(observe_data)\n",
    "observe_clear_u = np.concatenate((observe_clear_theta,observe_clear_w),axis=1)\n",
    "\n",
    "############# N(0,0.1^2) #############\n",
    "noise_nu = 0\n",
    "noise_std = 0.1\n",
    "noise_u = np.random.normal(loc=noise_nu, scale=noise_std, size=observe_clear_u.shape)\n",
    "observe_u = observe_clear_u + noise_u\n",
    "############# N(0,0.1^2) #############\n",
    "\n",
    "observe_data = numpy_to_tensor(observe_data, var_name=\"observe_data\", value_range_dim = True, to_torch = True, to_cuda = True, requires_grad = True)\n",
    "observe_clear_u = numpy_to_tensor(observe_clear_u, var_name=\"observe_u\", value_range_dim = True, to_torch = True, to_cuda = True, requires_grad = True)\n",
    "observe_u = numpy_to_tensor(observe_u, var_name=\"observe_u\", value_range_dim = True, to_torch = True, to_cuda = True, requires_grad = True)\n",
    "print('J:',len(observe_u))\n",
    "\n",
    "observe_data_x_inside = observe_data[:,0:1]\n",
    "observe_data_t_inside = observe_data[:,1:2]\n",
    "\n",
    "noise_u = numpy_to_tensor(noise_u, var_name=\"noise_u\", value_range_dim = True, to_torch = True, to_cuda = True, requires_grad = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e279491b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T08:41:42.558226Z",
     "start_time": "2025-01-28T08:41:42.545580Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(5678)\n",
    "N_test_number = 10000\n",
    "\n",
    "test_data = lb+(la-lb)*lhs(2,N_test_number)\n",
    "test_theta = exact_theta(test_data)\n",
    "test_w = exact_w(test_data)\n",
    "test_u = np.concatenate((test_theta,test_w),axis=1)\n",
    "\n",
    "test_data = numpy_to_tensor(test_data, var_name=\"test_data\", value_range_dim = True, to_torch = True, to_cuda = True, requires_grad = True)\n",
    "test_u = numpy_to_tensor(test_u, var_name=\"test_u\", value_range_dim = True, to_torch = True, to_cuda = True, requires_grad = True)\n",
    "\n",
    "test_data_x_inside = test_data[:,0:1]\n",
    "test_data_t_inside = test_data[:,1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d14a570",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T08:41:42.563803Z",
     "start_time": "2025-01-28T08:41:42.560341Z"
    }
   },
   "outputs": [],
   "source": [
    "C1 = torch.tensor(0.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44a970f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T08:41:42.570103Z",
     "start_time": "2025-01-28T08:41:42.565964Z"
    }
   },
   "outputs": [],
   "source": [
    "print('J:',len(observe_u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cff204f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T08:41:42.579424Z",
     "start_time": "2025-01-28T08:41:42.571862Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_loss_f(x_grad,t_grad,PINNs,C,return_sequence='not'):\n",
    "    \n",
    "    ########### loss f  ###########\n",
    "    E_inside = PINNs(torch.cat((x_grad,t_grad),axis=1))\n",
    "    \n",
    "    E_theta = E_inside[:,0:1]\n",
    "    E_w = E_inside[:,1:2]\n",
    "    \n",
    "    theta_tt = compute_higher_order_derivatives(E_theta, [t_grad,t_grad])\n",
    "    theta_x = compute_higher_order_derivatives(E_theta, [x_grad])\n",
    "    theta_xx = compute_higher_order_derivatives(theta_x, [x_grad])\n",
    "    \n",
    "    w_tt = compute_higher_order_derivatives(E_w, [t_grad,t_grad])\n",
    "    w_x = compute_higher_order_derivatives(E_w, [x_grad])\n",
    "    w_xx = compute_higher_order_derivatives(w_x, [x_grad])\n",
    "    \n",
    "    g = torch.cos(t_grad) - torch.pi/2 * torch.sin(x_grad) * torch.cos(t_grad)\n",
    "    \n",
    "    loss_f_1_sequence = C*theta_tt-theta_xx+(E_theta-w_x)\n",
    "    loss_f_1_squared_sequence = torch.square(loss_f_1_sequence)\n",
    "\n",
    "    loss_f_2_sequence = w_tt+theta_x-w_xx-g\n",
    "    loss_f_2_squared_sequence = torch.square(loss_f_2_sequence)\n",
    "    \n",
    "    if return_sequence=='yes':\n",
    "        return loss_f_1_squared_sequence,loss_f_2_squared_sequence\n",
    "    else:\n",
    "        return torch.mean(loss_f_1_squared_sequence)+torch.mean(loss_f_2_squared_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01256c56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T08:41:42.591789Z",
     "start_time": "2025-01-28T08:41:42.583518Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_loss_bound(bound_x, bound_x_t_zero, bound_t, PINNs, C, return_sequence='not'):\n",
    "    \n",
    "    E_bound_x_zero = PINNs(torch.cat((bound_x,bound_x_t_zero),axis=1)) #u(x,0) u(bound_x,t = bound_x_t_zero)\n",
    "    \n",
    "    E_theta_x_zero = E_bound_x_zero[:,0:1]  #theta(x,0)\n",
    "    E_w_x_zero = E_bound_x_zero[:,1:2] #w(x,0)\n",
    "    \n",
    "    E_theta_x_zero_dt = compute_higher_order_derivatives(E_theta_x_zero, [bound_x_t_zero]) #theta_t(x,0)\n",
    "    E_w_x_zero_dt = compute_higher_order_derivatives(E_w_x_zero, [bound_x_t_zero]) #w_t(x,0)\n",
    "    \n",
    "    loss_bound_x_a = E_theta_x_zero-(torch.pi/2)*torch.cos(bound_x)-(bound_x-torch.pi/2)\n",
    "    loss_bound_x_b = E_theta_x_zero_dt-torch.zeros_like(E_theta_x_zero_dt)\n",
    "    loss_bound_x_c = E_w_x_zero-(torch.pi/2)*torch.sin(bound_x)\n",
    "    loss_bound_x_d = E_w_x_zero_dt-torch.zeros_like(E_w_x_zero_dt)\n",
    "    \n",
    "    loss_bound_x = torch.mean(torch.square(loss_bound_x_a))+\\\n",
    "                   torch.mean(torch.square(loss_bound_x_b))+\\\n",
    "                   torch.mean(torch.square(loss_bound_x_c))+\\\n",
    "                   torch.mean(torch.square(loss_bound_x_d))\n",
    "\n",
    "    E_bound_zero_t = PINNs(torch.cat((torch.zeros_like(bound_t),bound_t),axis=1)) #u(0,t)\n",
    "    E_theta_zero_t = E_bound_zero_t[:,0:1]  #theta(0,t)\n",
    "    E_w_zero_t = E_bound_zero_t[:,1:2] #w(0,t)\n",
    "    \n",
    "    E_bound_pi_t = PINNs(torch.cat((torch.ones_like(bound_t)*torch.pi,bound_t),axis=1)) #u(pi,t)\n",
    "    E_theta_pi_t = E_bound_pi_t[:,0:1]  #theta(pi,t)\n",
    "    E_w_pi_t = E_bound_pi_t[:,1:2] #w(pi,t)\n",
    "    \n",
    "    loss_bound_t = torch.mean(torch.square(E_theta_zero_t))+\\\n",
    "                   torch.mean(torch.square(E_w_zero_t))+\\\n",
    "                   torch.mean(torch.square(E_theta_pi_t))+\\\n",
    "                   torch.mean(torch.square(E_w_pi_t))\n",
    "    \n",
    "    loss_bound_value = loss_bound_x+loss_bound_t\n",
    "    \n",
    "    return loss_bound_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5b00b9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T08:41:42.597045Z",
     "start_time": "2025-01-28T08:41:42.593663Z"
    }
   },
   "outputs": [],
   "source": [
    "#Paper reproduction\n",
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53946501",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T08:41:42.605686Z",
     "start_time": "2025-01-28T08:41:42.598961Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "net_settings_for_PINNs1 = NetSetting(input_dims=2, hidden_neurons_list=[50]*4, \n",
    "                                     output_dims=2, hidden_activation='tanh', \n",
    "                                     output_activation=None, initializer_method='xavier')\n",
    "PINNs1 = get_mlp_pinn(net_settings_for_PINNs1)\n",
    "PINNs1.cuda()\n",
    "\n",
    "initialize_weights(PINNs1, net_settings_for_PINNs1.initializer_method)\n",
    "\n",
    "optimizer1 = optim.Adam(PINNs1.parameters(), lr=0.001)    \n",
    "optimizer1.add_param_group({'params': [C1], 'lr': 0.001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2eb82fe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T08:41:42.611743Z",
     "start_time": "2025-01-28T08:41:42.608065Z"
    }
   },
   "outputs": [],
   "source": [
    "x_inside_all = torch.cat((x_inside,observe_data[:,0:1]),axis=0)\n",
    "t_inside_all = torch.cat((t_inside,observe_data[:,1:2]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f087e7d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T08:41:42.618590Z",
     "start_time": "2025-01-28T08:41:42.614323Z"
    }
   },
   "outputs": [],
   "source": [
    "Theta_list = np.zeros_like(observe_u.cpu().detach().numpy())\n",
    "Theta_list = numpy_to_tensor(Theta_list, var_name=\"Theta_list\", value_range_dim = True, to_torch = True, to_cuda = True, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55ca3f93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T08:41:42.624626Z",
     "start_time": "2025-01-28T08:41:42.620811Z"
    }
   },
   "outputs": [],
   "source": [
    "C1_id = id(C1)\n",
    "Theta_list_id = id(Theta_list)\n",
    "\n",
    "optimizer_error = optim.Adam([Theta_list], lr=0.0001,betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c95679c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T08:41:42.630713Z",
     "start_time": "2025-01-28T08:41:42.626835Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_optimizer1_error_correction(PINNs,C,Theta_list_p):\n",
    "    \n",
    "    for param in PINNs.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    C.requires_grad = True\n",
    "    \n",
    "    Theta_list_p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9a511a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T08:41:42.636930Z",
     "start_time": "2025-01-28T08:41:42.633101Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_optimizer2_error_correction(PINNs,C,Theta_list_p):\n",
    "    \n",
    "    for param in PINNs.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    C.requires_grad = False\n",
    "    \n",
    "    Theta_list_p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b61f37d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T08:41:42.644340Z",
     "start_time": "2025-01-28T08:41:42.639149Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_for_error_correction(PINNs,C,Theta_list_input,observe_data_input,observe_u_input):\n",
    "    \n",
    "    train_optimizer2_error_correction(PINNs,C,Theta_list_input)\n",
    "        \n",
    "    E_observation = PINNs(observe_data_input)\n",
    "    \n",
    "    loss_observation_refine = torch.mean(torch.square(E_observation+Theta_list_input-observe_u_input)) \n",
    "    \n",
    "    optimizer_error.zero_grad()\n",
    "    loss_observation_refine.backward()\n",
    "    optimizer_error.step()\n",
    "        \n",
    "    train_optimizer1_error_correction(PINNs,C,Theta_list_input)\n",
    "    \n",
    "    return Theta_list_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4523ed77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T08:41:42.650742Z",
     "start_time": "2025-01-28T08:41:42.646834Z"
    }
   },
   "outputs": [],
   "source": [
    "def flatten_line(data):\n",
    "    for i in range(1, len(data)):\n",
    "        if data[i] > data[i - 1]:\n",
    "            data[i] = data[i - 1]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be9c85b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T08:41:42.662057Z",
     "start_time": "2025-01-28T08:41:42.653156Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "def training_recall(PINNs, C, it_iter, loss_f_for_T_1_best, best_PINNs_state, \n",
    "                    loss_all_1, loss_f_1, loss_f_for_collocation_1, \n",
    "                    loss_f_for_T_1, loss_f_excapt_T_1, loss_T_1, \n",
    "                    loss_T_clear_1, loss_T_1_test_data,loss_T_EC_1, test_loss_1, \n",
    "                    C1_list, optimizer1):\n",
    "    \n",
    "    loss_f_for_T_1_smooth = flatten_line(loss_f_for_T_1.copy())\n",
    "    current_min_index = loss_f_for_T_1_smooth.index(min(loss_f_for_T_1_smooth))\n",
    "    current_min = min(loss_f_for_T_1_smooth)\n",
    "\n",
    "    if best_PINNs_state is None or current_min < best_PINNs_state['min_loss']:\n",
    "        best_PINNs_state = {\n",
    "            'model_state': copy.deepcopy(PINNs.state_dict()),\n",
    "            'C_state': C.clone().detach(),\n",
    "            'optimizer_state': copy.deepcopy(optimizer1.state_dict()),\n",
    "            'it': it_iter,\n",
    "            'min_loss': current_min\n",
    "        }\n",
    "\n",
    "    if len(loss_f_for_T_1_smooth) - current_min_index > 300:\n",
    "        print('Restoring best model from iteration:', best_PINNs_state['it'])\n",
    "        PINNs.load_state_dict(best_PINNs_state['model_state'])\n",
    "        \n",
    "        C_value = best_PINNs_state['C_state']\n",
    "        with torch.no_grad(): \n",
    "            C.copy_(C_value)  \n",
    "\n",
    "        optimizer1.load_state_dict(best_PINNs_state['optimizer_state'])\n",
    "        \n",
    "        recall_it = it_iter  \n",
    "        it_iter = best_PINNs_state['it']  \n",
    "\n",
    "        loss_all_1 = loss_all_1[:it_iter+1]\n",
    "        loss_f_1 = loss_f_1[:it_iter+1]\n",
    "        loss_f_for_collocation_1 = loss_f_for_collocation_1[:it_iter+1]\n",
    "        loss_f_for_T_1 = loss_f_for_T_1[:it_iter+1]\n",
    "        loss_f_excapt_T_1 = loss_f_excapt_T_1[:it_iter+1]\n",
    "        loss_T_1 = loss_T_1[:it_iter+1]\n",
    "        loss_T_clear_1 = loss_T_clear_1[:it_iter+1]\n",
    "        loss_T_1_test_data = loss_T_1_test_data[:it_iter+1]\n",
    "        test_loss_1 = test_loss_1[:it_iter+1]\n",
    "        C1_list = C1_list[:it_iter+1]\n",
    "        loss_T_EC_1 = loss_T_EC_1[:it_iter+1]\n",
    "        loss_f_for_T_1_best = True\n",
    "    \n",
    "    return (PINNs, C, it_iter, loss_f_for_T_1_best, best_PINNs_state, \n",
    "            loss_all_1, loss_f_1, loss_f_for_collocation_1, loss_f_for_T_1, \n",
    "            loss_f_excapt_T_1, loss_T_1, loss_T_clear_1, loss_T_1_test_data,loss_T_EC_1, \n",
    "            test_loss_1, C1_list,optimizer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed768c2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T08:41:42.679410Z",
     "start_time": "2025-01-28T08:41:42.664034Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AdaptiveChangeLROnPlateau:\n",
    "    def __init__(self, optimizer, patience_base=10, threshold_ratio = 1e-6, factor_down=0.1, patience_down=10, cooldown_down = 5, factor_up=1.1, patience_up=10, cooldown_up = 5, min_lr=0, max_lr=1, eps=1e-8):\n",
    "        self.optimizer = optimizer\n",
    "        self.patience_base = patience_base\n",
    "        self.factor_down = factor_down\n",
    "        self.patience_down = patience_down\n",
    "        self.cooldown_down = cooldown_down\n",
    "        self.factor_up = factor_up\n",
    "        self.patience_up = patience_up\n",
    "        self.cooldown_up = cooldown_up\n",
    "        \n",
    "        self.threshold_ratio = threshold_ratio\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.eps = eps\n",
    "        self.cooldown_counter = 0\n",
    "        \n",
    "        self.best_metrics = None\n",
    "        self.best_base_loss = None\n",
    "        \n",
    "        self.num_bad_epochs_base = 0\n",
    "        self.num_bad_epochs_up = 0\n",
    "        self.num_bad_epochs_down = 0\n",
    "        self.renew_iter = 0\n",
    "        \n",
    "    def step(self, loss_f_for_T_1_input, loss_f_for_collocation_1_input, to_iter):\n",
    "        \n",
    "        loss_f_for_T_1_input = np.array(loss_f_for_T_1_input[self.renew_iter:])\n",
    "        loss_f_for_collocation_1_input =  np.array(loss_f_for_collocation_1_input[self.renew_iter:])\n",
    "        \n",
    "        metrics = (loss_f_for_T_1_input)[-1]-flatten_line(loss_f_for_collocation_1_input)[-1] \n",
    "        base_loss = flatten_line(loss_f_for_T_1_input)[-1]    \n",
    "        \n",
    "        # Performance improvement logic\n",
    "        if (self.best_metrics is not None) and (self.best_base_loss is not None):\n",
    "\n",
    "            if base_loss == self.best_base_loss:\n",
    "                self.num_bad_epochs_base += 1\n",
    "                \n",
    "                if self.num_bad_epochs_base > self.patience_base:\n",
    "                    self.num_bad_epochs_base = 0\n",
    "                    self.num_bad_epochs_up = 0\n",
    "                    self.num_bad_epochs_down = 0\n",
    "                    self.renew_iter = to_iter\n",
    "                    \n",
    "                    for param_group in self.optimizer.param_groups:\n",
    "                        param_group['lr'] = self.max_lr\n",
    "\n",
    "                    self._adjust_C_lr(self.factor_down**2, \"down\")\n",
    "\n",
    "            if metrics  >=  self.best_metrics + self.threshold_ratio:\n",
    "                self.num_bad_epochs_up += 1\n",
    "                \n",
    "                if self.num_bad_epochs_up > self.patience_up:\n",
    "                    if self.cooldown_counter > 0:\n",
    "                        self.cooldown_counter -= 1\n",
    "                    else:\n",
    "                        self._adjust_lr(self.factor_up, \"up\")\n",
    "                        #print(\"up\")\n",
    "                        self.cooldown_counter = self.cooldown_up\n",
    "                        self.num_bad_epochs_base = 0\n",
    "                        self.num_bad_epochs_up = 0\n",
    "                        self.num_bad_epochs_down = 0\n",
    "\n",
    "            elif metrics  < self.best_metrics - self.threshold_ratio:\n",
    "                self.num_bad_epochs_down += 1\n",
    "                \n",
    "                if self.num_bad_epochs_down > self.patience_down:\n",
    "                    if self.cooldown_counter > 0:\n",
    "                        self.cooldown_counter -= 1\n",
    "                    else:\n",
    "                        self._adjust_lr(self.factor_down, \"down\")\n",
    "                        #print(\"down\")\n",
    "                        self.cooldown_counter = self.cooldown_down\n",
    "                        self.num_bad_epochs_base = 0\n",
    "                        self.num_bad_epochs_up = 0\n",
    "                        self.num_bad_epochs_down = 0\n",
    "\n",
    "        self.best_metrics = metrics\n",
    "        self.best_base_loss = base_loss\n",
    "        \n",
    "    def _adjust_lr(self, factor, direction):\n",
    "        \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            old_lr = param_group['lr']\n",
    "            \n",
    "            if old_lr == 0:\n",
    "                old_lr = self.min_lr\n",
    "                \n",
    "            if direction == \"down\":\n",
    "                new_lr = old_lr * factor\n",
    "                if new_lr<self.min_lr:\n",
    "                    new_lr = 0\n",
    "            else:\n",
    "                new_lr = min(old_lr * factor, self.max_lr)\n",
    "                \n",
    "            param_group['lr'] = new_lr\n",
    "\n",
    "    def _adjust_C_lr(self, factor, direction):\n",
    "        \n",
    "        for param_group in optimizer1.param_groups:\n",
    "            if any(id(param) == C1_id for param in param_group['params']):\n",
    "                old_lr = param_group['lr']\n",
    "\n",
    "                if old_lr == 0:\n",
    "                    old_lr = self.min_lr\n",
    "\n",
    "                if direction == \"down\":\n",
    "                    new_lr = old_lr * factor\n",
    "                    if new_lr<=self.min_lr:\n",
    "                        new_lr = 0\n",
    "\n",
    "                param_group['lr'] = new_lr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c75f8775",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T08:41:42.685597Z",
     "start_time": "2025-01-28T08:41:42.681770Z"
    }
   },
   "outputs": [],
   "source": [
    "scheduler = AdaptiveChangeLROnPlateau(optimizer_error, patience_base=100, threshold_ratio = 1e-6,\n",
    "                                      factor_down=0.1, patience_down=50, cooldown_down = 0, \n",
    "                                      factor_up=10, patience_up=50, cooldown_up = 0,\n",
    "                                      min_lr=1e-8, max_lr=1e-4, eps=1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1fd135",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T08:58:11.718426Z",
     "start_time": "2025-01-28T08:41:42.687987Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############## Record list ###############\n",
    "loss_all_1 = []\n",
    "loss_f_1 = []\n",
    "loss_f_for_collocation_1 = []\n",
    "loss_f_for_T_1 = []\n",
    "loss_f_excapt_T_1 = []\n",
    "loss_T_1 = []\n",
    "loss_T_clear_1 = []\n",
    "loss_T_1_test_data = []\n",
    "loss_T_EC_1 = []\n",
    "test_loss_1 = []\n",
    "C1_list = []\n",
    "############## Record list ###############\n",
    "\n",
    "\n",
    "nIter1 = 20000\n",
    "it = 0\n",
    "best_total_loss = 100\n",
    "best_test_loss = 100 \n",
    "\n",
    "# Initial placeholders for best model and iteration\n",
    "loss_f_for_T_1_best = False\n",
    "best_PINNs_state = None\n",
    "best_it = None\n",
    "recall_it = None\n",
    "\n",
    "while it<nIter1:\n",
    "    #########loss f#########\n",
    "    loss_f = get_loss_f(x_inside_all,t_inside_all,PINNs1,C1,return_sequence='not')\n",
    "    \n",
    "    #########loss f  for collocation data#########\n",
    "    loss_f_for_collocation = get_loss_f(x_inside,t_inside,PINNs1,C1,return_sequence='not')\n",
    "    #########loss f  for observation data#########\n",
    "    loss_f_for_T = get_loss_f(observe_data_x_inside,observe_data_t_inside,PINNs1,C1,return_sequence='not')\n",
    "    #########loss f  excapt observation data#########\n",
    "    loss_f_excapt_T = get_loss_f(test_data_x_inside,test_data_t_inside,PINNs1,C1,return_sequence='not')\n",
    "    \n",
    "    #########loss b#########        \n",
    "    loss_b = get_loss_bound(x_bound, x_bound_t_zero, t_bound, PINNs1, C1, return_sequence='not')\n",
    "    \n",
    "    #########loss T observation#########        \n",
    "    E_observation = PINNs1(observe_data) \n",
    "    \n",
    "    if loss_f_for_T_1_best:\n",
    "        loss_observation = torch.mean(torch.square(E_observation+Theta_list-observe_u)) \n",
    "    else:\n",
    "        loss_observation = torch.mean(torch.square(E_observation-observe_u))            \n",
    "\n",
    "    #########loss EC######### \n",
    "    loss_EC = torch.mean(torch.square(Theta_list-noise_u))   \n",
    "    \n",
    "    #########loss T noisy observation#########  \n",
    "    loss_observation_noisy = torch.mean(torch.square(E_observation-observe_u)) \n",
    "    #########loss T clear observation#########        \n",
    "    E_observation_clear = PINNs1(observe_data) \n",
    "    loss_observation_clear = torch.mean(torch.square(E_observation_clear-observe_clear_u))    \n",
    "    #########loss T excapt observation#########        \n",
    "    E_observation_excapt = PINNs1(test_data) \n",
    "    loss_observation_excapt = torch.mean(torch.square(E_observation_excapt-test_u))   \n",
    "    \n",
    "    #########loss PI#########\n",
    "    loss = loss_f+loss_b+10*loss_observation\n",
    "    \n",
    "    #########test_loss NRMSE#########\n",
    "    pre_u = PINNs1(test_data)\n",
    "    test_loss = relative_l2_torch(pre_u,test_u)\n",
    "    #########test_loss NRMSE#########\n",
    "    \n",
    "    #########Record#########\n",
    "    loss_f_1.append(loss_f.item())\n",
    "    loss_f_for_collocation_1.append(loss_f_for_collocation.item())\n",
    "    loss_f_for_T_1.append(loss_f_for_T.item())\n",
    "    loss_f_excapt_T_1.append(loss_f_excapt_T.item())\n",
    "    C1_list.append(C1.item())   \n",
    "    loss_T_1.append(loss_observation_noisy.item()) \n",
    "    loss_T_EC_1.append(loss_EC.item())  \n",
    "    loss_T_clear_1.append(loss_observation_clear.item()) \n",
    "    loss_T_1_test_data.append(loss_observation_excapt.item()) \n",
    "    test_loss_1.append(test_loss)\n",
    "    #########Record#########\n",
    "       \n",
    "    if it % 1000 == 0:\n",
    "        \n",
    "        print('It:', it, 'train_loss:', loss.item(), 'test_loss:', test_loss)\n",
    "        \n",
    "        for param_group in optimizer_error.param_groups:\n",
    "            if any(id(param) == Theta_list_id for param in param_group['params']):\n",
    "                Theta_list_lr = param_group['lr']\n",
    "                print(f'Current Theta_list learning rate: {Theta_list_lr}')\n",
    "                \n",
    "        for param_group in optimizer1.param_groups:\n",
    "            if any(id(param) == C1_id for param in param_group['params']):\n",
    "                C1_lr = param_group['lr']\n",
    "                print(f'Current C1 learning rate: {C1_lr}')\n",
    "                \n",
    "        print('C1',C1)   \n",
    "        \n",
    "    optimizer1.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer1.step()  \n",
    "        \n",
    "    if  not loss_f_for_T_1_best:\n",
    "        if it>0:\n",
    "            (PINNs1, C1, it, loss_f_for_T_1_best, best_PINNs_state, \n",
    "             loss_all_1, loss_f_1, loss_f_for_collocation_1, loss_f_for_T_1, \n",
    "             loss_f_excapt_T_1, loss_T_1, loss_T_clear_1, loss_T_1_test_data,loss_T_EC_1, \n",
    "             test_loss_1, C1_list, optimizer1) = training_recall(\n",
    "                 PINNs1, C1, it, loss_f_for_T_1_best, best_PINNs_state, \n",
    "                 loss_all_1, loss_f_1, loss_f_for_collocation_1, \n",
    "                 loss_f_for_T_1, loss_f_excapt_T_1, loss_T_1, loss_T_clear_1, \n",
    "                 loss_T_1_test_data,loss_T_EC_1, test_loss_1, C1_list, optimizer1)\n",
    "            \n",
    "            if loss_f_for_T_1_best:\n",
    "                Theta_list.requires_grad = True\n",
    "                for param_group in optimizer1.param_groups:\n",
    "                    if any(id(param) == C1_id for param in param_group['params']):\n",
    "                        param_group['lr'] = 0.0001  \n",
    "    else:\n",
    "        ############################ Refine L_T ############################   \n",
    "        scheduler.step(loss_f_for_T_1,loss_f_for_collocation_1,it)\n",
    "        Theta_list = train_for_error_correction(PINNs1,C1,Theta_list,observe_data,observe_u)\n",
    "\n",
    "    it = it + 1   \n",
    "    \n",
    "    \n",
    "    ############################ Save the Best Model ############################\n",
    "    if it>19500 and loss.item() < best_total_loss:\n",
    "        best_total_loss = loss.item()\n",
    "        torch.save(PINNs1.state_dict(), '../saved_model/Total_AdaEC_model.pth')\n",
    "        print(f\"New best model saved at iteration {it}\")\n",
    "        \n",
    "    if it>19500 and test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        torch.save(PINNs1.state_dict(), '../saved_model/Test_AdaEC_model.pth')\n",
    "        print(f\"New best model saved at iteration {it}\")\n",
    "    ########################################################################### \n",
    "    \n",
    "print('Final:', 'train_loss:', loss.item(), 'test_loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c08143",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GA-PINNs",
   "language": "python",
   "name": "gapings"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
